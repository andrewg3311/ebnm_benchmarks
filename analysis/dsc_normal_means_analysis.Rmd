---
title: "Normal Means DSC Analysis"
author: "Andrew Goldstein"
date: "October 30, 2018"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = T, warning = T, message = F, collapse = T, comment = "#", results = 'hold', fig.align = 'center', fig.height = 8)
```

# Introduction
This code analyzes the results of the DSC replications. Currently, only the `point_normal` prior is tested. 

The `point_normal` prior assumes
$$
\begin{aligned}
\theta_j \stackrel{iid}{\sim} \pi_0 \delta_\mu + (1 - \pi_0)\mathcal{N}\Big(\mu, \frac1a\Big) \\
x_j \stackrel{\perp}{\sim} \mathcal{N}\Big(\theta_j, s_j^2\Big)
\end{aligned}
$$
By setting $\mu := 0$, we can recover the case where there is a point-mass at 0. By setting $\pi_0 := 0$, we can recover the case where there is no point-mass.

# Analysis
```{r}
# Load required packages
library(dscrutils)
library(dplyr)
library(tidyr)
library(ggplot2)

# load and reshape data
dscout = dscquery(dsc.outdir = "./code/dsc-normalmeans/normal_means", 
                  targets = c("simulate.true_pi0", "simulate.true_a", "simulate.true_mu", 
                              "eb.prior", "eb.fix_mu", "eb.g_pi0", "eb.g_a", "eb.g_mu", 
                              "eb.loglik", "score.score"))

dscout_wide = dscout %>% spread(score, score.score)
View(dscout_wide)
```
We replicate 10 times for all combinations of parameter values (with the same seeds for consistency): $\mu \in \{0\}$, $\pi_0 \in \{0, .2, .5, .8, 1\}$, $a \in \{1/25, 1/16, 1/4\}$, and we try both estimating $\mu$ and fixing $\mu := 0$. The standard errors $s_j$ were all set to 1.

## Parameter Estimation
In this section, we investigate our ability to recover the parameter values.

Figures 1-3 below show our attempts at recovering $a$, $\pi_0$, and $\mu$, respectively. For figures 1-2, the top and bottom rows separate between the cases when $\mu$ was fixed at 0 (bottom) or estimated (top). For figure 3, only the cases where $\mu$ was estimated are shown.
```{r}
labels_pi0 = character(length(unique(dscout_wide$simulate.true_pi0)))
for (i in 1:length(labels_pi0)) {
  labels_pi0[i] = paste("pi0 = ", unique(dscout_wide$simulate.true_pi0)[i], sep = "")
  names(labels_pi0)[i] = unique(dscout_wide$simulate.true_pi0)[i]
}
labels_a = character(length(unique(dscout_wide$simulate.true_a)))
for (i in 1:length(labels_a)) {
  labels_a[i] = paste("a = ", unique(dscout_wide$simulate.true_a)[i], sep = "")
  names(labels_a)[i] = unique(dscout_wide$simulate.true_a)[i]
}
labels_fix_mu = character(length(unique(dscout_wide$eb.fix_mu)))
for (i in 1:length(labels_fix_mu)) {
  labels_fix_mu[i] = paste("fix_mu = ", unique(dscout_wide$eb.fix_mu)[i], sep = "")
  names(labels_fix_mu)[i] = unique(dscout_wide$eb.fix_mu)[i]
}

ggplot(dscout_wide, aes(x = factor(simulate.true_a), y = eb.g_a)) +
  geom_boxplot() + facet_grid(eb.fix_mu ~ simulate.true_pi0, labeller = labeller(simulate.true_pi0 = labels_pi0, eb.fix_mu = labels_fix_mu)) +
  ggtitle("Figure 1a: Recovering precision a") + xlab("True a") + ylab ("Estimated a")

ggplot(filter(dscout_wide, simulate.true_pi0 != 1), aes(x = factor(simulate.true_a), y = eb.g_a)) +
  geom_boxplot() + facet_grid(eb.fix_mu ~ simulate.true_pi0, labeller = labeller(simulate.true_pi0 = labels_pi0, eb.fix_mu = labels_fix_mu)) +
  ggtitle("Figure 1b: Recovering precision a") + xlab("True a") + ylab ("Estimated a")

ggplot(dscout_wide, aes(x = factor(simulate.true_pi0), y = eb.g_pi0)) +
  geom_boxplot() + facet_grid(eb.fix_mu ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, eb.fix_mu = labels_fix_mu))+ 
  ggtitle("Figure 2: Recovering null component pi0") + xlab("True pi0") + ylab ("Estimated pi0")

ggplot(filter(dscout_wide, eb.fix_mu == 0), aes(x = factor(simulate.true_mu), y = eb.g_mu)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 3: Recovering mean mu") + xlab("True mu") + ylab ("Estimated mu")
```

From these plots, a few things become apparent. First, figure 1a shows that for larger values of $\pi_0$, our estimates of $a$ can become highly inaccurate and extremely inflated. This is due to the estimated $\widehat{\pi_0}$ being very slightly smaller than 1, which can result in inflated values of $a$. I recommend that we add a check for this condition.

(Figure 1b removes the case where $\pi_0 = 1$, so the other cases can be seen more clearly).

Second, when we simultaneously estimate $\mu$, our estimate for $\pi_0$ can suffer in the case when $\pi_0 = 1$.

Third, when estimating $\pi_0$, all outliers are below the true value for $\pi_0$, which suggests the method is more likely to be anti-conservative for estimating $\pi_0$.

Fourth, when estimating $\mu$, the method appears to return sensible estimates.

## RMSE of Estimating Effects $\theta_j$
In this section, we look at the RMSEs of estimating the true effects, $\theta_j$, using the posterior means supplied by the `ebnm` function.

Figure 4 below plots the RMSEs and MADs for all combinations of $\pi_0$, $a$, and whether or not we fixed $\mu := 0$.
```{r}
ggplot(dscout_wide, aes(x = factor(eb.fix_mu), y = score_theta_RMSE)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 4a: Estimating Effects (Posterior Mean RMSE)") + xlab("Fix_mu") + ylab ("RMSE")

ggplot(dscout_wide, aes(x = factor(eb.fix_mu), y = score_theta_MAD)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 4b: Estimating Effects (Posterior Mean MAD)") + xlab("Fix_mu") + ylab ("MAD")
```
We can see that fixing $\mu := 0$ doesn't have much of an effect on the RMSE (with the obvious exception of when all effects are null, so our RMSE is 0). As expected, the RMSE is inversely related to $\pi_0$, and inversely related to $a$.

Figure 5 below plots the RMSEs and MADs for the MLE estimate, $\hat{\theta_j} = x_j$. Recall that the standard deviations were $s_j := 1$.
```{r}
ggplot(dscout_wide, aes(y = score_MLE_RMSE)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 5a: Estimating Effects (MLE RMSE)") + xlab("") + ylab ("RMSE") + 
  theme(axis.text.x = element_blank())

ggplot(dscout_wide, aes(y = score_MLE_MAD)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 5b: Estimating Effects (MLE MAD)") + xlab("") + ylab ("MAD") + 
  theme(axis.text.x = element_blank())
```

We can see that the posterior mean out-performs the MLE in all cases, when comparing with both RMSE and MAD.

## Log-Likelihoods
In this section, we look at the log-likelihoods returned from the `ebnm` function.

Figure 6 below plots the log-likelihoods for all combinations of $\pi_0$, $a$, and whether or not we fixed $\mu := 0$.
```{r}
ggplot(dscout_wide, aes(x = factor(eb.fix_mu), y = eb.loglik)) +
  geom_boxplot() + facet_grid(simulate.true_pi0 ~ simulate.true_a, labeller = labeller(simulate.true_a = labels_a, simulate.true_pi0 = labels_pi0)) +
  ggtitle("Figure 6: Log-Likelihoods") + xlab("Fix_mu") + ylab ("log-likelihood")
```
We can see that estimating $\mu$ has little to no effect on the resulting log-likelihood.

