---
title: "EBNM Benchmarking"
author: "Andrew Goldstein"
date: "October 16, 2018"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = F, warning = F, message = F, results = 'asis', fig.align = 'center')
```

# Introduction
This code executes the functions in the `ebnm` package on simulated data. We simulate data according to different generating processes, and test the speed and accuracy of `ebnm_normal`, `ebnm_point_normal`, and `ebnm_point_laplace` functions.

```{r}
# Load required packages
library(workflowr)
library(microbenchmark)
devtools::load_all("../ebnm")
```

# `ebnm_normal` Testing
In this section, we test the `ebnm_normal` function. This function assumes that data are generated from the following model:
$$
\begin{aligned}
\theta_j \stackrel{iid}{\sim} \mathcal{N}\Big(\mu, \frac1a\Big) \\
x_j \stackrel{\perp}{\sim} \mathcal{N}\Big(\theta_j, s_j^2\Big)
\end{aligned}
$$
The $x_j$ and $s_j$ are supplied to the `enbm_normal` function, which then finds the MLE estimates $\hat{\mu}$ and $\hat{a}$ and provides posterior estimates for first and second moments of $\theta_j$.
